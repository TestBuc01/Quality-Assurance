# CHAPTER 1

# Terms Set 1.1



> &#x1F539; Defect - a flaw in a component or system that can cause the component or system to fail to 	perform its required function, an incorrect statement or data definition.
	- mistakes makes by people during the design and built.

> &#x1F539; Error - a human action that produces an incorrect result.

> &#x1F539; Failure - deviation the component or system from its expected delivery, service or result.

> &#x1F539; Quality - the degree to which a component, system or process meets specified 		  	requirements and/or user/customer needs and expectations.
	     - depends by the number of the defects founded
	     - is measured by looking at the attributes of the product
	     - is based on good manufacturing processes, and meeting defined requirements

> &#x1F539; Risk	 - is future uncertain events with a probability of occurrence and a potential for 	loss
	 - is something that has not happened yet and it may never happen; it is a 		potential problem

> &#x1F539; Software - computer programs, procedures, and possibly associated documentation and data 	pertaining to the operation of a computer system.

> &#x1F539; Testing - the process consisting of all lifecycle activities, both static and dynamic, 		concerned with planning, preparation and evaluation of software products and 		related work products to determine that they satisfy specified requirements, to 	demonstrate that they are fit for purpose and to detect defects.

> &#x1F539; Exhaustive testing - a test approach in which the test suite comprises all combinations of input values and preconditions.



# Terms Set 1.2

> &#x1F539; Debugging - the process of finding, analyzing and removing the causes of failures in software.

> &#x1F539; Requirement - a condition or capability needed by an user to solve a problem or achieve an objective that must be met or possessed by a system or system component to satisfy 	a contract, standard, specification, or other formally imposed document.

> &#x1F539; Review - an evaluation of a product or project status to ascertain discrepancies from planned results and to recommend improvements. Examples include management review, 	informal review, technical review, inspection, and wallkthrough.

> &#x1F539; Test basis - All documents from which the requirements of a component or system can be inferred. The documentation on which the test cases are based. If a document can be amended only by way of formal amendment procedure, then the test basis is called a frozen test basis.

> &#x1F539; Test case - A set of input values, execution preconditions, expected results and execution postconditions, developed for a particular objective or test condition, such as to exercise a particular program path or to verify compliance with a specific requirement.

> &#x1F539; Testing - The process consisting of all lifecycle activities, both static and dynamic, concerned with planning, preparation and evaluation of software products and related work products to determine that they satisfy specified requirements, to demonstrate that they are fit for purpose and to detect defects.


> &#x1F539; Test objective - A reason or purpose for designing and executing a test.



# Terms Set 1.3

> &#x1F539; Testing shows presence of defects - Testing can show that defects are present, but cannot prove that there are no defects. Testing reduces the probability of undiscovered defects remaining in the software but, even if no defects are found, it is not a proof of correctness.

> &#x1F539; Exhaustive testing is impossible - Testing everything (all combinations of inputs and preconditions) is not feasible except for trivial cases. Instead of exhaustive testing, we use risks and priorities to focus testing efforts.

> &#x1F539; Early testing - Testing activities should start as early as possible in the software or system development life cycle and should be focused on defined objectives.

> &#x1F539; Defect clustering - A small number of modules contain most of the defects discovered during pre-release testing or show the most operational failures.

> &#x1F539; Pesticide paradox - If the same tests are repeated over and over again, eventually the same set of test cases will no longer find any new bugs. To overcome this 'pesticide paradox', the test cases need to be regularly reviewed and revised, and new and different tests need to be written to exercise different parts of the software or system to potentially find more defects.

> &#x1F539; Testing is context dependent - Testing is done differently in different contexts. For example, safety-critical software is tested differently from an e-commerce site.   

 
> &#x1F539; Absence-of-errors fallacy - Finding and fixing defects does not help if the system built is unusable and does not fulfill the users' needs and expectations.


# Terms Set 1.4


> &#x1F539; Confirmation testing - Testing that runs test cases that failed the last time they were run, in order to verify the success of corrective actions.

> &#x1F539; Exit criteria - The set of generic and specific conditions, agreed upon with the stakeholders for permitting a process to be officially completed. The purpose of exit criteria is to prevent a task from being considered completed when there are still outstanding parts of the task which have not been finished. Exit criteria are used to report against and to plan when to stop testing.

> &#x1F539; Incident - Any event occurring that requires investigation.

> &#x1F539; Regression testing - Testing of a previously tested program following modification to ensure that defects have not been introduced or uncovered in unchanged areas of the software, as a result of the changes made. It is performed when the software or its environment is changed.

> &#x1F539; Test basis - All documents from which the requirements of a component or system can be inferred. The documentation on which the test cases are based. If a document can be amended only by way of formal amendment procedure, then the test basis is called a frozen test basis.

> &#x1F539; Test condition - An item or event of a component or system that could be verified by one or more test cases, e.g., a function, transaction, feature, quality attribute, or structural element.

> &#x1F539; Test coverage - The degree, expressed as a percentage, to which a specified coverage item has been exercised by a test suite.

> &#x1F539; Test data - Data that exists (for example, in a database) before a test is executed, and that affects or is affected by the component or system under test.

> &#x1F539; Test execution - The process of running a test on the component or system under test, producing actual result(s).

> &#x1F539; Test log - A chronological record of relevant details about the execution of tests.

> &#x1F539; Test plan - A document describing the scope, approach, resources and schedule of intended test activities. It identifies amongst others test items, the features to be tested, the testing tasks, who will do each task, degree of tester independence, the test environment, the test design techniques and entry and exit criteria to be used, and the rationale for their choice, and any risks requiring contingency planning. It is a record of the test planning process.

> &#x1F539; Test strategy - A high-level description of the test levels to be performed and the testing within those levels for an organization or programmer (one or more projects).

> &#x1F539; Test summary report - A document summarizing testing activities and results. It also contains an evaluation of the corresponding test items against exit criteria.

> &#x1F539; Testware - Artifacts produced during the test process required to plan, design, and execute tests, such as documentation, scripts, inputs, expected results, set-up and clear-up procedures, files, databases, environment, and any additional software or utilities used in testing.

# Terms Set 1.5

> &#x1F539; Independence - Separation of responsibilities, which encourages the accomplishment of objective testing.
